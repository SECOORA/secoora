{
 "metadata": {
  "name": "",
  "signature": "sha256:a381fd0344148ba62833aaa0b75905e6137cc50d3efaa72dd8b1fb26aadd15df"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### SECOORA iundation notebook\n",
      "Based on IOOS system-test [notebook](https://github.com/ioos/system-test/tree/master/Theme_2_Extreme_Events/Scenario_2A_Coastal_Inundation/Scenario_2A_ModelDataCompare_Inundation)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pytz\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "# Choose the date range.\n",
      "if False:\n",
      "    stop = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)\n",
      "else:\n",
      "    stop = datetime(2014, 8, 8, 12)\n",
      "\n",
      "stop = stop.replace(tzinfo=pytz.utc)\n",
      "start = stop - timedelta(days=7)\n",
      "\n",
      "# SECOORA region (NC, SC GA, FL).\n",
      "bounding_box = [-87.40, 24.25, -74.70, 36.70]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Start logging"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging as log\n",
      "reload(log)\n",
      "\n",
      "log.captureWarnings(True)\n",
      "LOG_FILENAME = '{:%Y-%m-%d}-{}'.format(start, 'secoora_inundation.log')\n",
      "log.basicConfig(filename=LOG_FILENAME,\n",
      "                filemode='w',\n",
      "                format='%(asctime)s %(levelname)s: %(message)s',\n",
      "                datefmt='%I:%M:%S',\n",
      "                level=log.INFO,\n",
      "                stream=None)\n",
      "\n",
      "log.info('Run date: {:%Y-%m-%d %H:%M:%S}'.format(datetime.utcnow()))\n",
      "log.info('Download start: {:%Y-%m-%d %H:%M:%S}'.format(start))\n",
      "log.info('Download stop: {:%Y-%m-%d %H:%M:%S}'.format(stop))\n",
      "log.info('Bounding box: {0:3.2f}, {1:3.2f}, {2:3.2f}, {3:3.2f}'.format(*bounding_box))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from owslib import fes\n",
      "from utilities import date_range_filter\n",
      "\n",
      "# CF-names to look for (Sea Surface Height).\n",
      "name_list = ['water level',\n",
      "             'sea_surface_height',\n",
      "             'sea_surface_elevation',\n",
      "             'sea_surface_height_above_geoid',\n",
      "             'sea_surface_height_above_sea_level',\n",
      "             'water_surface_height_above_reference_datum',\n",
      "             'sea_surface_height_above_reference_ellipsoid']\n",
      "\n",
      "kw = dict(wildCard='*',\n",
      "          escapeChar='\\\\',\n",
      "          singleChar='?',\n",
      "          propertyname='apiso:AnyText')\n",
      "\n",
      "or_filt = fes.Or([fes.PropertyIsLike(literal=('*%s*' % val), **kw)\n",
      "                  for val in name_list])\n",
      "\n",
      "not_filt = fes.Not([fes.PropertyIsLike(literal='*Averages*', **kw)])\n",
      "\n",
      "bbox = fes.BBox(bounding_box)\n",
      "\n",
      "begin, end = date_range_filter(start, stop)\n",
      "filter_list = [fes.And([bbox, begin, end, or_filt, not_filt])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from owslib.csw import CatalogueServiceWeb\n",
      "\n",
      "endpoint = 'http://www.ngdc.noaa.gov/geoportal/csw'\n",
      "csw = CatalogueServiceWeb(endpoint, timeout=60)\n",
      "csw.getrecords2(constraints=filter_list, maxrecords=1000, esn='full')\n",
      "\n",
      "log.info(\"CSW version: %s\" % csw.version)\n",
      "log.info(\"Number of datasets available: %s\" % len(csw.records.keys()))"
     ],
     "language": "python",
     "metadata": {
      "code_folding": []
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utilities import service_urls\n",
      "\n",
      "dap_urls = service_urls(csw.records, service='odp:url')\n",
      "sos_urls = service_urls(csw.records, service='sos:url')\n",
      "\n",
      "for rec, item in csw.records.items():\n",
      "    log.info('CSW: {}'.format(item.title))\n",
      "\n",
      "for url in dap_urls:\n",
      "    log.info('DAP: {}.html'.format(url))\n",
      "\n",
      "for url in sos_urls:\n",
      "    log.info('SOS: {}'.format(url))"
     ],
     "language": "python",
     "metadata": {
      "code_folding": []
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyoos.collectors.coops.coops_sos import CoopsSos\n",
      "\n",
      "collector = CoopsSos()\n",
      "sos_name = 'water_surface_height_above_reference_datum'\n",
      "\n",
      "datum = 'NAVD'\n",
      "collector.set_datum(datum)\n",
      "collector.end_time = stop\n",
      "collector.start_time = start\n",
      "collector.variables = [sos_name]\n",
      "\n",
      "ofrs = collector.server.offerings\n",
      "\n",
      "log.info('{}: {} offerings'.format(collector.server.identification.title, len(ofrs)))"
     ],
     "language": "python",
     "metadata": {
      "code_folding": []
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import read_csv\n",
      "from utilities import sos_request\n",
      "\n",
      "params = dict(observedProperty=sos_name,\n",
      "              eventTime=start.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
      "              featureOfInterest='BBOX:{0},{1},{2},{3}'.format(*bounding_box),\n",
      "              offering = 'urn:ioos:network:NOAA.NOS.CO-OPS:WaterLevelActive')\n",
      "\n",
      "url = sos_request(url='http://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS', **params)\n",
      "observations = read_csv(url)\n",
      "\n",
      "log.info('sos_request'.format(url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Clean the dataframe"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "from utilities import get_coops_longname, table_style\n",
      "\n",
      "columns = {'datum_id': 'datum',\n",
      "           'sensor_id': 'sensor',\n",
      "           'station_id': 'station',\n",
      "           'latitude (degree)': 'lat',\n",
      "           'longitude (degree)': 'lon',\n",
      "           'vertical_position (m)': 'height',\n",
      "           'water_surface_height_above_reference_datum (m)': 'ssh above datum'}\n",
      "\n",
      "observations.rename(columns=columns, inplace=True)\n",
      "\n",
      "observations['datum'] = [s.split(':')[-1] for s in observations['datum']]\n",
      "observations['sensor'] = [s.split(':')[-1] for s in observations['sensor']]\n",
      "observations['station'] = [s.split(':')[-1] for s in observations['station']]\n",
      "observations['name'] = [get_coops_longname(s) for s in observations['station']]\n",
      "\n",
      "observations.set_index('name', inplace=True)\n",
      "HTML(table_style +  observations.head().to_html(classes='df'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Generate a uniform 6-min time base for model/data comparison"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import iris\n",
      "iris.FUTURE.netcdf_promote = True\n",
      "from iris.pandas import as_data_frame\n",
      "from pandas import DataFrame\n",
      "from utilities import coops2df, save_timeseries_cube\n",
      "from owslib.ows import ExceptionReport\n",
      "\n",
      "fname = '{:%Y-%m-%d}-OBS_DATA.nc'.format(start)\n",
      "if not os.path.isfile(fname):\n",
      "    data = dict()\n",
      "    for s in observations.station:\n",
      "        try:\n",
      "            b = coops2df(collector, s, sos_name)\n",
      "            b = b['water_surface_height_above_reference_datum (m)']\n",
      "            data.update({s: b})\n",
      "        except ExceptionReport as e:\n",
      "            log.warning(\"[%s] %s:\\n%s\" % (s, get_coops_longname(s), e))\n",
      "    obs_data = DataFrame.from_dict(data)\n",
      "\n",
      "    diff = set(observations['station'].values).difference(obs_data.columns)\n",
      "    non_navd = [c not in diff for c in observations['station']]\n",
      "    observations = observations[non_navd]\n",
      "\n",
      "    comment = \"Several stations from http://opendap.co-ops.nos.noaa.gov\"\n",
      "    kw = dict(longitude=observations.lon,\n",
      "              latitude=observations.lat,\n",
      "              station_attr=dict(cf_role=\"timeseries_id\"),\n",
      "              cube_attr=dict(featureType='timeSeries',\n",
      "                             Conventions='CF-1.6',\n",
      "                             standard_name_vocabulary='CF-1.6',\n",
      "                             cdm_data_type=\"Station\",\n",
      "                             comment=comment,\n",
      "                             datum=datum,\n",
      "                             url=url))\n",
      "\n",
      "    save_timeseries_cube(obs_data, outfile=fname, **kw)\n",
      "else:\n",
      "    cube = iris.load_cube(fname)\n",
      "    cube.remove_coord('longitude')\n",
      "    cube.remove_coord('latitude')\n",
      "    obs_data = as_data_frame(cube)\n",
      "\n",
      "HTML(table_style +  obs_data.head().to_html(max_cols=10, classes='df'))"
     ],
     "language": "python",
     "metadata": {
      "code_folding": []
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Loop the models and save a csv series at each station"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from iris.pandas import as_series\n",
      "from iris.exceptions import (CoordinateNotFoundError, CoordinateMultiDimError,\n",
      "                             ConstraintMismatchError, MergeError)\n",
      "\n",
      "import folium\n",
      "import vincent\n",
      "import numpy as np\n",
      "from pandas import Series\n",
      "\n",
      "# Local imports.\n",
      "from utilities import make_tree, get_coordinates, get_model_name, get_nearest_water, service_urls, slice_bbox_extract, plt_grid, get_cube"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [],
      "input_collapsed": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "name_in_list = lambda cube: cube.standard_name in name_list\n",
      "constraint = iris.Constraint(cube_func=name_in_list, coord_values=None)\n",
      "\n",
      "dfs = dict()\n",
      "for url in dap_urls:\n",
      "    if 'NECOFS' in url:  # FIXME: NECOFS has cartesian coordinates.\n",
      "        continue\n",
      "    try:\n",
      "        cube = get_cube(url, constraint, jd_start, jd_stop)\n",
      "    except (RuntimeError, ValueError, ConstraintMismatchError) as e:\n",
      "        print('Cannot get cube for: %s\\n%s' % (url, e))\n",
      "        continue\n",
      "    try:\n",
      "        longitude = bounding_box[0][0], bounding_box[1][0]\n",
      "        latitude = bounding_box[0][1], bounding_box[1][1]\n",
      "        cube = cube.intersection(longitude=longitude, latitude=latitude)\n",
      "    except CoordinateMultiDimError:\n",
      "        cube = slice_bbox_extract(cube, bounding_box)\n",
      "\n",
      "    mod_name, model_full_name = get_model_name(cube, url)\n",
      "    print('\\n[%s] %s:\\n%s\\n' % (mod_name, cube.attributes['title'], url))\n",
      "\n",
      "    fname = '%s.nc' % mod_name\n",
      "    if not os.path.isfile(fname):\n",
      "        try:  # Make tree.\n",
      "            tree, lon, lat = make_tree(cube)\n",
      "            fig, ax = plt_grid(lon, lat)\n",
      "        except CoordinateNotFoundError as e:\n",
      "            print('Cannot make KDTree for: %s' % mod_name)\n",
      "            continue\n",
      "        # Get model series at observed locations.\n",
      "        raw_series = dict()\n",
      "        interp_series = dict()\n",
      "        for station, obs in observations.iterrows():\n",
      "            a = obs_data[obs['station']]\n",
      "            try:\n",
      "                kw = dict(k=10, max_dist=0.04, min_var=0.01)\n",
      "                series, dist, idx = get_nearest_water(cube, tree,\n",
      "                                                      obs.lon, obs.lat, **kw)\n",
      "            except ValueError as e:\n",
      "                print(e)\n",
      "                continue\n",
      "            if not series:\n",
      "                status = \"Not Found\"\n",
      "                series = Series(np.empty_like(a) * np.NaN, index=a.index)\n",
      "            else:\n",
      "                raw_series.update({obs['station']: series})\n",
      "                series = as_series(series)\n",
      "                status = \"Found\"\n",
      "                ax.plot(lon[idx], lat[idx], 'g.')\n",
      "\n",
      "            print('[%s] %s' % (status, obs.name))\n",
      "\n",
      "            kw = dict(method='time')\n",
      "            series = series.reindex(a.index).interpolate(**kw).ix[a.index]\n",
      "            interp_series.update({obs['station']: series})\n",
      "\n",
      "        interp_series = DataFrame.from_dict(interp_series).dropna(axis=1, how='all')\n",
      "        dfs.update({fname[:-3]: interp_series})\n",
      "\n",
      "        if raw_series:  # Save cube.\n",
      "            for station, cube in raw_series.items():\n",
      "                station_coord = iris.coords.AuxCoord(station,\n",
      "                                                     var_name=\"station\",\n",
      "                                                     long_name=\"station name\")\n",
      "                cube.add_aux_coord(station_coord)\n",
      "\n",
      "            try:\n",
      "                cube = iris.cube.CubeList(raw_series.values()).merge_cube()\n",
      "            except MergeError:\n",
      "                cube = iris.cube.CubeList(raw_series.values()).merge()\n",
      "            iris.save(cube, fname)\n",
      "\n",
      "        ax.set_title('%s: Points found %s' % (mod_name, len(interp_series.columns)))\n",
      "        ax.plot(observations.lon, observations.lat, 'ro',\n",
      "                zorder=1, label='Observation', alpha=0.25)\n",
      "        ax.set_extent([bounding_box[0][0], bounding_box[1][0],\n",
      "                       bounding_box[0][1], bounding_box[1][1]])"
     ],
     "language": "python",
     "metadata": {
      "code_folding": []
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import Panel\n",
      "\n",
      "dfs.update(OBS_DATA=obs_data)\n",
      "\n",
      "dfs = Panel.fromDict(dfs)\n",
      "dfs = dfs.swapaxes(0, 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inundation_map = folium.Map(location=[np.mean(bounding_box, axis=0)[1],\n",
      "                                      np.mean(bounding_box, axis=0)[0]],\n",
      "                            zoom_start=5)\n",
      "\n",
      "# Create the map and add the bounding box line.\n",
      "inundation_map.line(get_coordinates(bounding_box, bounding_box_type),\n",
      "                    line_color='#FF0000', line_weight=2)\n",
      "\n",
      "html = '<b>Station:</b><br>%s<br><b>Long Name:</b><br>%s'\n",
      "\n",
      "for station in dfs:\n",
      "    sta_name = get_coops_longname(station)\n",
      "    df = dfs[station].dropna(axis=1, how='all')\n",
      "    # FIXME: This is bad!  But I cannot represent NaN with Vega!\n",
      "    df.fillna(value=0, inplace=True)\n",
      "    vis = vincent.Line(df, width=400, height=200)\n",
      "    vis.axis_titles(x='Time', y='Sea surface height (m)')\n",
      "    vis.legend(title=sta_name)\n",
      "    vis.name = sta_name\n",
      "    json = 'station_%s.json' % station\n",
      "    vis.to_json(json)\n",
      "    obs = observations[observations['station'] == station]\n",
      "    inundation_map.simple_marker(location=[obs['lat'].values[0],\n",
      "                                           obs['lon'].values[0]],\n",
      "                                 popup=(vis, json))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inundation_map.create_map(path='inundation_map.html')\n",
      "inundation_map.render_iframe = True\n",
      "inundation_map"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(LOG_FILENAME, 'rt') as f:\n",
      "    print('Log file:\\n')\n",
      "    print(body)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}